{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/birdslab/Desktop/Krithikaa/fluxes/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import joblib\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to load the datasets, preprocess and format the data\n",
    "\n",
    "\n",
    "Input: None\n",
    "\n",
    "\n",
    "Output: Return the preprocessed dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "\n",
    "    #load T and P\n",
    "    t = pd.read_csv('../data/compare_transcriptome.csv')\n",
    "    p = pd.read_csv('../data/compare_proteome.csv')\n",
    "\n",
    "    #remove the NaN values from P\n",
    "    p.dropna(inplace=True)\n",
    "\n",
    "    #set the knockout name as the index\n",
    "    t.index = t['Unnamed: 0']\n",
    "    p.index = p['Unnamed: 0']\n",
    "\n",
    "    #remove the knockout name column from the dataframe\n",
    "    t = t.iloc[:, 1:]\n",
    "    p = p.iloc[:, 2:]\n",
    "\n",
    "    #load the fluxomics datasets and set the index same as T or P\n",
    "    fluxes_p = pd.read_csv('../fluxomics/fluxes/Fluxes_glpk_seed1.csv', header=None)\n",
    "    fluxes_p = fluxes_p.T\n",
    "    fluxes_p.index = p.index \n",
    "\n",
    "    fluxes_t = pd.read_csv('../fluxomics/fluxes/Fluxes_t_glpk_seed1.csv', header=None)\n",
    "    fluxes_t  = fluxes_t.T\n",
    "    fluxes_t.index = t.index\n",
    "\n",
    "    #min-max normalization of fluxomics dataframe   \n",
    "    fluxes_p = (fluxes_p - fluxes_p.min()) / (fluxes_p.max() - fluxes_p.min())\n",
    "\n",
    "    fluxes_t = (fluxes_t - fluxes_t.min()) / (fluxes_t.max() - fluxes_t.min())\n",
    "\n",
    "    #remove the NaN columns from the normalized fluxomics dataframe\n",
    "    fluxes_p.dropna(axis=1, inplace=True)\n",
    "    fluxes_t.dropna(axis=1, inplace=True)\n",
    "\n",
    "    #min-max normalization of T and P dataframe\n",
    "    p = (p - p.min())/(p.max() - p.min())\n",
    "    t = (t - t.min())/(t.max() - t.min())\n",
    "\n",
    "    #load the growth rates and set the index same as T or P\n",
    "    gr_measured_ML = pd.read_csv('../data/compare_transcriptome_measured_gr.csv')\n",
    "    gr_measured_ML.index = t.index\n",
    "\n",
    "    y = pd.read_csv('../data/compare_measured_gr.csv')\n",
    "    y.index = y['ORF PROT']\n",
    "\n",
    "    return [t, fluxes_t, p, fluxes_p, gr_measured_ML, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to filter knockouts as low or high growth based on a given percentile\n",
    "\n",
    "\n",
    "Input: Liquid GR, Solid GR, Percentile for splitting\n",
    "\n",
    "\n",
    "Output: Modified GR dataframe with a class column [0 as low and 2 as high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ko(gr_measured_ML, y, a_per, b_per):\n",
    "    \n",
    "    #find the percentile for liquid GR\n",
    "    a = np.percentile(gr_measured_ML['0'], a_per)\n",
    "    b = np.percentile(gr_measured_ML['0'], b_per)\n",
    "\n",
    "    y_t = []\n",
    "    for i in gr_measured_ML['0']:\n",
    "        if i < a:\n",
    "            y_t.append(0)\n",
    "        elif i>a and i<b:\n",
    "            y_t.append(1)\n",
    "        else:\n",
    "            y_t.append(2)\n",
    "\n",
    "    #append class column based on the calculated percentile\n",
    "    gr_measured_ML['class'] = y_t\n",
    "\n",
    "\n",
    "    #find the percentile for solid GR\n",
    "    a = np.percentile(y['SM'], a_per)\n",
    "    b = np.percentile(y['SM'], b_per)\n",
    "\n",
    "    y_p = []\n",
    "    for i in y['SM']:\n",
    "        if i < a:\n",
    "            y_p.append(0)\n",
    "        elif i>a and i<b:\n",
    "            y_p.append(1)\n",
    "        else:\n",
    "            y_p.append(2)\n",
    "\n",
    "    #append class column based on the calculated percentile\n",
    "    y['class'] = y_p\n",
    "\n",
    "    #filter medium gr KOs\n",
    "    gr_measured_ML = gr_measured_ML[gr_measured_ML['class'] != 1]\n",
    "    y = y[y['class'] != 1]\n",
    "\n",
    "    return [gr_measured_ML, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to combine the transcriptomics, proteomics and fluxomics dataset into a single dataframe\n",
    "\n",
    "\n",
    "Input: Transcriptomics, Proteomics, Fluxomics derived from transcriptomics, Fluxomics derived from proteomics\n",
    "\n",
    "\n",
    "Output: Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combined_dataset(t, p, fluxes_t, fluxes_p):\n",
    "\n",
    "    X = pd.concat([t, fluxes_t, p, fluxes_p], axis=1)\n",
    "    X.columns = X.columns.astype(str)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to split the dataset into train, test and validation \n",
    "\n",
    "Input: y dataframes\n",
    "\n",
    "Output: Train, test and validation dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(y):\n",
    "\n",
    "    # est: 30% of the total data\n",
    "    test_size = int(0.3*y.shape[0])\n",
    "\n",
    "    #train: 80% of the remaining data\n",
    "    train_size = int(0.8*(y.shape[0] - test_size))\n",
    "\n",
    "    valid_size = int(0.2*(y.shape[0] - test_size))\n",
    "\n",
    "    #randomly sample train_size number of rows, ensuring equal split of each class\n",
    "    y_train = y.groupby('class', group_keys=False).apply(lambda x: x.sample(n=train_size//2, random_state=42))\n",
    "\n",
    "    #filter the remaining indices\n",
    "    train_valid = y.loc[list(set(y.index).difference(set(y_train.index))), :]\n",
    "\n",
    "    #randomly sample valid_size number of rows from the remaining rows\n",
    "    y_valid = train_valid.groupby('class', group_keys=False).apply(lambda x: x.sample(n=valid_size//2, random_state=42))\n",
    "\n",
    "    y_test = y.loc[list(set(y.index).difference(set(y_valid.index).union(set(y_train.index)))), :]\n",
    "\n",
    "    return [y_train, y_test, y_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract features from the models\n",
    "\n",
    "Input: model, dataframe with the gene/protein names, x validation dataframe, file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(model, labels, X_valid, file_name, svm=False):\n",
    "\n",
    "    if not svm:\n",
    "        explainer = shap.Explainer(model.best_estimator_)\n",
    "        shap_values = explainer.shap_values(X_valid)\n",
    "        shap_summary = [np.mean(i) for i in np.abs(shap_values[0])]\n",
    "        shap_summary_pd = pd.DataFrame({'Feature': X_valid.columns, 'SHAP values': shap_summary})\n",
    "        shap_summary_pd = shap_summary_pd.sort_values('SHAP values', ascending=False)\n",
    "        shap_summary_pd['Feature'] = [labels[labels['Gene Names (ordered locus)'] == i]['Entry'].values[0] for i in shap_summary_pd['Feature']]\n",
    "        shap_summary_pd.to_csv(\"./pathway analysis/\" + file_name + \".csv\")\n",
    "    else:\n",
    "\n",
    "        #initialize the shap explainer\n",
    "        explainer = shap.Explainer(model.best_estimator_.predict, X_valid)\n",
    "\n",
    "        #calculate the shap values\n",
    "        shap_values = explainer.shap_values(X_valid)\n",
    "        shap_summary = [np.mean(i) for i in np.abs(shap_values[0])]\n",
    "\n",
    "        #convert the values to a dataframe\n",
    "        shap_summary_pd = pd.DataFrame({'Feature': X_valid.columns, 'SHAP values': shap_summary})\n",
    "\n",
    "        #sort in ascending order\n",
    "        shap_summary_pd = shap_summary_pd.sort_values('SHAP values', ascending=False)\n",
    "\n",
    "        #set gene name for the features\n",
    "        shap_summary_pd['Feature'] = [labels[labels['Gene Names (ordered locus)'] == i]['Entry'].values[0] for i in shap_summary_pd['Feature']]\n",
    "        \n",
    "        return shap_summary_pd\n",
    "        # #save the features as a csv\n",
    "        # shap_summary_pd.to_csv(\"./pathway analysis/\" + file_name + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/birdslab/Desktop/Krithikaa/fluxes/.venv/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "labels = pd.read_excel(\"../data/gene-protein-yeast.xlsx\")\n",
    "double_KO = pd.read_csv(\"../data/yeast_gstf_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation importance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_train = y.groupby('class', group_keys=False).apply(lambda x: x.sample(n=train_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_valid = train_valid.groupby('class', group_keys=False).apply(lambda x: x.sample(n=valid_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_train = y.groupby('class', group_keys=False).apply(lambda x: x.sample(n=train_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_valid = train_valid.groupby('class', group_keys=False).apply(lambda x: x.sample(n=valid_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_train = y.groupby('class', group_keys=False).apply(lambda x: x.sample(n=train_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_valid = train_valid.groupby('class', group_keys=False).apply(lambda x: x.sample(n=valid_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_train = y.groupby('class', group_keys=False).apply(lambda x: x.sample(n=train_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_valid = train_valid.groupby('class', group_keys=False).apply(lambda x: x.sample(n=valid_size//2, random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_train = y.groupby('class', group_keys=False).apply(lambda x: x.sample(n=train_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_valid = train_valid.groupby('class', group_keys=False).apply(lambda x: x.sample(n=valid_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_train = y.groupby('class', group_keys=False).apply(lambda x: x.sample(n=train_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_valid = train_valid.groupby('class', group_keys=False).apply(lambda x: x.sample(n=valid_size//2, random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_train = y.groupby('class', group_keys=False).apply(lambda x: x.sample(n=train_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_valid = train_valid.groupby('class', group_keys=False).apply(lambda x: x.sample(n=valid_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_train = y.groupby('class', group_keys=False).apply(lambda x: x.sample(n=train_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_valid = train_valid.groupby('class', group_keys=False).apply(lambda x: x.sample(n=valid_size//2, random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_train = y.groupby('class', group_keys=False).apply(lambda x: x.sample(n=train_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_valid = train_valid.groupby('class', group_keys=False).apply(lambda x: x.sample(n=valid_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_train = y.groupby('class', group_keys=False).apply(lambda x: x.sample(n=train_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_valid = train_valid.groupby('class', group_keys=False).apply(lambda x: x.sample(n=valid_size//2, random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "coef = pd.DataFrame()\n",
    "splits = [10, 20, 30, 40, 50]\n",
    "\n",
    "for s in splits:\n",
    "        \n",
    "        #load and preprocess data\n",
    "        [t, fluxes_t, p, fluxes_p, gr_measured_ML, y] = load_dataset()\n",
    "\n",
    "        percent = s\n",
    "\n",
    "        #filter based on percentile\n",
    "        [gr_measured_ML, y] = filter_ko(gr_measured_ML, y, percent, 100 - percent)\n",
    "\n",
    "        #concatenate the dataframes\n",
    "        X = generate_combined_dataset(t, p, fluxes_t, fluxes_p)\n",
    "\n",
    "        #split transcriptomics dataset into train, test and validation\n",
    "        [y_train_liq, y_test_liq, y_valid_liq] = split_data(gr_measured_ML)\n",
    "        X_train_liq = X.loc[y_train_liq.index, :]\n",
    "        X_test_liq = X.loc[y_test_liq.index, :]\n",
    "        X_valid_liq = X.loc[y_valid_liq.index, :]\n",
    "\n",
    "        y_valid_liq = y_valid_liq['class']\n",
    "        y_train_liq = y_train_liq['class']\n",
    "        y_test_liq = y_test_liq['class']\n",
    "\n",
    "        #split proteomics dataset into train, test and validation\n",
    "        [y_train_solid, y_test_solid, y_valid_solid] = split_data(y)\n",
    "        X_train_solid = X.loc[y_train_solid.index, :]\n",
    "        X_test_solid = X.loc[y_test_solid.index, :]\n",
    "        X_valid_solid = X.loc[y_valid_solid.index, :]\n",
    "\n",
    "        y_train_solid = y_train_solid['class']\n",
    "        y_test_solid = y_test_solid['class']\n",
    "        y_valid_solid = y_valid_solid['class']\n",
    "\n",
    "        print(s)\n",
    "\n",
    "        #subset train and validation for transcriptomics\n",
    "        x_train = X_train_liq.iloc[:, 0:1826].loc[:, double_KO.columns[1:]]\n",
    "        x_valid = X_valid_liq.iloc[:, 0:1826].loc[:, double_KO.columns[1:]]\n",
    "\n",
    "        loaded_model = joblib.load(\"svm_t_glpk_\"+str(percent)+\".joblib\")\n",
    "        result = permutation_importance(loaded_model, x_valid, y_valid_liq, n_repeats=10, random_state=0, n_jobs=-1)\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: result.importances_mean[i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"knn_t_glpk_\"+str(percent)+\".joblib\")\n",
    "        result = permutation_importance(loaded_model, x_valid, y_valid_liq, n_repeats=10, random_state=0, n_jobs=-1)\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: result.importances_mean[i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"lasso_log_t_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: loaded_model.best_estimator_.coef_[0][i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"elasticnet_log_t_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: loaded_model.best_estimator_.coef_[0][i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "\n",
    "        transcriptome = X_train_liq.iloc[:, 0:1826].loc[:, double_KO.columns[1:]]\n",
    "        flux = X_train_liq.iloc[:, 1826: 3776]\n",
    "        x_train = pd.concat([transcriptome, flux], axis=1)\n",
    "        x_valid = X_valid_liq.iloc[:, 0:3776].loc[:, x_train.columns]\n",
    "\n",
    "        loaded_model = joblib.load(\"svm_t_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        result = permutation_importance(loaded_model, x_valid, y_valid_liq, n_repeats=10, random_state=0, n_jobs=-1)\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: result.importances_mean[i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"knn_t_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        result = permutation_importance(loaded_model, x_valid, y_valid_liq, n_repeats=10, random_state=0, n_jobs=-1)\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: result.importances_mean[i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"lasso_log_t_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: loaded_model.best_estimator_.coef_[0][i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"elasticnet_log_t_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: loaded_model.best_estimator_.coef_[0][i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "\n",
    "        x_train = X_train_solid.iloc[:, 3776:5602].loc[:, double_KO.columns[1:]]\n",
    "        x_valid = X_valid_solid.iloc[:, 3776:5602].loc[:, double_KO.columns[1:]]\n",
    "        x_train.dropna(inplace=True)\n",
    "        x_valid.dropna(inplace=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"svm_p_glpk_\"+str(percent)+\".joblib\")\n",
    "        result = permutation_importance(loaded_model, x_valid, y_valid_solid.loc[x_valid.index], n_repeats=10, random_state=0, n_jobs=-1)\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: result.importances_mean[i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"knn_p_glpk_\"+str(percent)+\".joblib\")\n",
    "        result = permutation_importance(loaded_model, x_valid, y_valid_solid.loc[x_valid.index], n_repeats=10, random_state=0, n_jobs=-1)\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: result.importances_mean[i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"lasso_log_p_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: loaded_model.best_estimator_.coef_[0][i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"elasticnet_log_p_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: loaded_model.best_estimator_.coef_[0][i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "\n",
    "        proteome = X_train_solid.iloc[:, 3776:5602].loc[:, double_KO.columns[1:]]\n",
    "        flux = X_train_solid.iloc[:, 5602: ]\n",
    "        x_train = pd.concat([proteome, flux], axis=1)\n",
    "        x_valid = X_valid_solid.iloc[:, 3776:].loc[:, x_train.columns]\n",
    "        x_train.dropna(inplace=True)\n",
    "        x_valid.dropna(inplace=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"svm_p_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        result = permutation_importance(loaded_model, x_valid, y_valid_solid.loc[x_valid.index], n_repeats=10, random_state=0, n_jobs=-1)\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: result.importances_mean[i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"knn_p_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        result = permutation_importance(loaded_model, x_valid, y_valid_solid.loc[x_valid.index], n_repeats=10, random_state=0, n_jobs=-1)\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: result.importances_mean[i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "        \n",
    "        loaded_model = joblib.load(\"lasso_log_p_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: loaded_model.best_estimator_.coef_[0][i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"elasticnet_log_p_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: loaded_model.best_estimator_.coef_[0][i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "for p in [10, 20, 30, 40, 50]:\n",
    "    for data in ['t', 't_f', 'p', 'p_f']:\n",
    "        for j in ['svm', 'knn', 'lasso_log','elasticnet_log']:\n",
    "            index.append(j+'_'+data+'_glpk_'+str(p))\n",
    "\n",
    "coef.index = index\n",
    "coef.to_csv(\"classifiers_features_permutation_imp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_train = y.groupby('class', group_keys=False).apply(lambda x: x.sample(n=train_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_valid = train_valid.groupby('class', group_keys=False).apply(lambda x: x.sample(n=valid_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_train = y.groupby('class', group_keys=False).apply(lambda x: x.sample(n=train_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_27149/3380375233.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_valid = train_valid.groupby('class', group_keys=False).apply(lambda x: x.sample(n=valid_size//2, random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "coef_r = pd.DataFrame()\n",
    "splits = [50]\n",
    "\n",
    "for s in splits:\n",
    "        \n",
    "        #load and preprocess data\n",
    "        [t, fluxes_t, p, fluxes_p, gr_measured_ML, y] = load_dataset()\n",
    "\n",
    "        percent = s\n",
    "\n",
    "        #filter based on percentile\n",
    "        [gr_measured_ML, y] = filter_ko(gr_measured_ML, y, percent, 100 - percent)\n",
    "\n",
    "        #concatenate the dataframes\n",
    "        X = generate_combined_dataset(t, p, fluxes_t, fluxes_p)\n",
    "\n",
    "        #split transcriptomics dataset into train, test and validation\n",
    "        [y_train_liq, y_test_liq, y_valid_liq] = split_data(gr_measured_ML)\n",
    "        X_train_liq = X.loc[y_train_liq.index, :]\n",
    "        X_test_liq = X.loc[y_test_liq.index, :]\n",
    "        X_valid_liq = X.loc[y_valid_liq.index, :]\n",
    "\n",
    "        y_valid_liq = y_valid_liq['0']\n",
    "        y_train_liq = y_train_liq['0']\n",
    "        y_test_liq = y_test_liq['0']\n",
    "\n",
    "        #split proteomics dataset into train, test and validation\n",
    "        [y_train_solid, y_test_solid, y_valid_solid] = split_data(y)\n",
    "        X_train_solid = X.loc[y_train_solid.index, :]\n",
    "        X_test_solid = X.loc[y_test_solid.index, :]\n",
    "        X_valid_solid = X.loc[y_valid_solid.index, :]\n",
    "\n",
    "        y_train_solid = y_train_solid['SM']\n",
    "        y_test_solid = y_test_solid['SM']\n",
    "        y_valid_solid = y_valid_solid['SM']\n",
    "\n",
    "        print(s)\n",
    "\n",
    "        #subset train and validation for transcriptomics\n",
    "        x_train = X_train_liq.iloc[:, 0:1826].loc[:, double_KO.columns[1:]]\n",
    "        x_valid = X_valid_liq.iloc[:, 0:1826].loc[:, double_KO.columns[1:]]\n",
    "\n",
    "        loaded_model = joblib.load(\"svm_reg_t_glpk_\"+str(percent)+\".joblib\")\n",
    "        result = permutation_importance(loaded_model, x_valid, y_valid_liq, n_repeats=10, random_state=0, n_jobs=-1)\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: result.importances_mean[i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef_r = pd.concat([coef_r, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"lasso_reg_t_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: loaded_model.best_estimator_.coef_[i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef_r = pd.concat([coef_r, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "\n",
    "        transcriptome = X_train_liq.iloc[:, 0:1826].loc[:, double_KO.columns[1:]]\n",
    "        flux = X_train_liq.iloc[:, 1826: 3776]\n",
    "        x_train = pd.concat([transcriptome, flux], axis=1)\n",
    "        x_valid = X_valid_liq.iloc[:, 0:3776].loc[:, x_train.columns]\n",
    "\n",
    "        loaded_model = joblib.load(\"svm_reg_t_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        result = permutation_importance(loaded_model, x_valid, y_valid_liq, n_repeats=10, random_state=0, n_jobs=-1)\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: result.importances_mean[i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef_r = pd.concat([coef_r, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"lasso_reg_t_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: loaded_model.best_estimator_.coef_[i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef_r = pd.concat([coef_r, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "\n",
    "        x_train = X_train_solid.iloc[:, 3776:5602].loc[:, double_KO.columns[1:]]\n",
    "        x_valid = X_valid_solid.iloc[:, 3776:5602].loc[:, double_KO.columns[1:]]\n",
    "        x_train.dropna(inplace=True)\n",
    "        x_valid.dropna(inplace=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"svm_reg_p_glpk_\"+str(percent)+\".joblib\")\n",
    "        result = permutation_importance(loaded_model, x_valid, y_valid_solid.loc[x_valid.index], n_repeats=10, random_state=0, n_jobs=-1)\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: result.importances_mean[i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef_r = pd.concat([coef_r, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"lasso_reg_p_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: loaded_model.best_estimator_.coef_[i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef_r = pd.concat([coef_r, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "\n",
    "        proteome = X_train_solid.iloc[:, 3776:5602].loc[:, double_KO.columns[1:]]\n",
    "        flux = X_train_solid.iloc[:, 5602: ]\n",
    "        x_train = pd.concat([proteome, flux], axis=1)\n",
    "        x_valid = X_valid_solid.iloc[:, 3776:].loc[:, x_train.columns]\n",
    "        x_train.dropna(inplace=True)\n",
    "        x_valid.dropna(inplace=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"svm_reg_p_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        result = permutation_importance(loaded_model, x_valid, y_valid_solid.loc[x_valid.index], n_repeats=10, random_state=0, n_jobs=-1)\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: result.importances_mean[i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef_r = pd.concat([coef_r, pd.DataFrame([features])], ignore_index=True)\n",
    "        \n",
    "        loaded_model = joblib.load(\"lasso_reg_p_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = {loaded_model.best_estimator_.feature_names_in_[i]: loaded_model.best_estimator_.coef_[i] for i in range(len(loaded_model.best_estimator_.feature_names_in_))}\n",
    "        coef_r = pd.concat([coef_r, pd.DataFrame([features])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "for p in [50]:\n",
    "    for data in ['t', 't_f', 'p', 'p_f']:\n",
    "        for j in ['svm', 'lasso_reg']:\n",
    "            index.append(j+'_'+data+'_glpk_'+str(p))\n",
    "\n",
    "coef_r.index = index\n",
    "coef_r.to_csv(\"regressors_features_permutation_imp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.DataFrame()\n",
    "splits = [10, 20, 30, 40, 50]\n",
    "\n",
    "for s in splits:\n",
    "        coef = pd.DataFrame()\n",
    "        \n",
    "        #load and preprocess data\n",
    "        [t, fluxes_t, p, fluxes_p, gr_measured_ML, y] = load_dataset()\n",
    "\n",
    "        percent = s\n",
    "\n",
    "        #filter based on percentile\n",
    "        [gr_measured_ML, y] = filter_ko(gr_measured_ML, y, percent, 100 - percent)\n",
    "\n",
    "        #concatenate the dataframes\n",
    "        X = generate_combined_dataset(t, p, fluxes_t, fluxes_p)\n",
    "\n",
    "        #split transcriptomics dataset into train, test and validation\n",
    "        [y_train_liq, y_test_liq, y_valid_liq] = split_data(gr_measured_ML)\n",
    "        X_train_liq = X.loc[y_train_liq.index, :]\n",
    "        X_test_liq = X.loc[y_test_liq.index, :]\n",
    "        X_valid_liq = X.loc[y_valid_liq.index, :]\n",
    "\n",
    "        y_valid_liq = y_valid_liq['class']\n",
    "        y_train_liq = y_train_liq['class']\n",
    "        y_test_liq = y_test_liq['class']\n",
    "\n",
    "        #split proteomics dataset into train, test and validation\n",
    "        [y_train_solid, y_test_solid, y_valid_solid] = split_data(y)\n",
    "        X_train_solid = X.loc[y_train_solid.index, :]\n",
    "        X_test_solid = X.loc[y_test_solid.index, :]\n",
    "        X_valid_solid = X.loc[y_valid_solid.index, :]\n",
    "\n",
    "        y_train_solid = y_train_solid['class']\n",
    "        y_test_solid = y_test_solid['class']\n",
    "        y_valid_solid = y_valid_solid['class']\n",
    "\n",
    "        print(s)\n",
    "\n",
    "        #subset train and validation for transcriptomics\n",
    "        x_train = X_train_liq.iloc[:, 0:1826].loc[:, double_KO.columns[1:]]\n",
    "        x_valid = X_valid_liq.iloc[:, 0:1826].loc[:, double_KO.columns[1:]]\n",
    "\n",
    "        loaded_model = joblib.load(\"svm_t_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model.best_estimator_, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"knn_t_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"lasso_log_t_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"elasticnet_log_t_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "\n",
    "        transcriptome = X_train_liq.iloc[:, 0:1826].loc[:, double_KO.columns[1:]]\n",
    "        flux = X_train_liq.iloc[:, 1826: 3776]\n",
    "        x_train = pd.concat([transcriptome, flux], axis=1)\n",
    "        x_valid = X_valid_liq.iloc[:, 0:3776].loc[:, x_train.columns]\n",
    "\n",
    "        loaded_model = joblib.load(\"svm_t_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model.best_estimator_, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"knn_t_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"lasso_log_t_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"elasticnet_log_t_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "\n",
    "        x_train = X_train_solid.iloc[:, 3776:5602].loc[:, double_KO.columns[1:]]\n",
    "        x_valid = X_valid_solid.iloc[:, 3776:5602].loc[:, double_KO.columns[1:]]\n",
    "        x_train.dropna(inplace=True)\n",
    "        x_valid.dropna(inplace=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"svm_p_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model.best_estimator_, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"knn_p_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"lasso_log_p_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"elasticnet_log_p_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "\n",
    "        proteome = X_train_solid.iloc[:, 3776:5602].loc[:, double_KO.columns[1:]]\n",
    "        flux = X_train_solid.iloc[:, 5602: ]\n",
    "        x_train = pd.concat([proteome, flux], axis=1)\n",
    "        x_valid = X_valid_solid.iloc[:, 3776:].loc[:, x_train.columns]\n",
    "        x_train.dropna(inplace=True)\n",
    "        x_valid.dropna(inplace=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"svm_p_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model.best_estimator_, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"knn_p_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "        \n",
    "        loaded_model = joblib.load(\"lasso_log_p_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"elasticnet_log_p_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "\n",
    "        index = []\n",
    "        for data in ['t', 't_f', 'p', 'p_f']:\n",
    "                for j in ['svm', 'knn', 'lasso_log','elasticnet_log']:\n",
    "                        index.append(j+'_'+data+'_glpk_'+str(p))\n",
    "\n",
    "        coef.index = index\n",
    "        coef.to_csv(\"classifiers_features_shap\"+str(s)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.DataFrame()\n",
    "splits = [50]\n",
    "\n",
    "for s in splits:\n",
    "        coef = pd.DataFrame()\n",
    "        \n",
    "        #load and preprocess data\n",
    "        [t, fluxes_t, p, fluxes_p, gr_measured_ML, y] = load_dataset()\n",
    "\n",
    "        percent = s\n",
    "\n",
    "        #filter based on percentile\n",
    "        [gr_measured_ML, y] = filter_ko(gr_measured_ML, y, percent, 100 - percent)\n",
    "\n",
    "        #concatenate the dataframes\n",
    "        X = generate_combined_dataset(t, p, fluxes_t, fluxes_p)\n",
    "\n",
    "        #split transcriptomics dataset into train, test and validation\n",
    "        [y_train_liq, y_test_liq, y_valid_liq] = split_data(gr_measured_ML)\n",
    "        X_train_liq = X.loc[y_train_liq.index, :]\n",
    "        X_test_liq = X.loc[y_test_liq.index, :]\n",
    "        X_valid_liq = X.loc[y_valid_liq.index, :]\n",
    "\n",
    "        y_valid_liq = y_valid_liq['class']\n",
    "        y_train_liq = y_train_liq['class']\n",
    "        y_test_liq = y_test_liq['class']\n",
    "\n",
    "        #split proteomics dataset into train, test and validation\n",
    "        [y_train_solid, y_test_solid, y_valid_solid] = split_data(y)\n",
    "        X_train_solid = X.loc[y_train_solid.index, :]\n",
    "        X_test_solid = X.loc[y_test_solid.index, :]\n",
    "        X_valid_solid = X.loc[y_valid_solid.index, :]\n",
    "\n",
    "        y_train_solid = y_train_solid['class']\n",
    "        y_test_solid = y_test_solid['class']\n",
    "        y_valid_solid = y_valid_solid['class']\n",
    "\n",
    "        print(s)\n",
    "\n",
    "        #subset train and validation for transcriptomics\n",
    "        x_train = X_train_liq.iloc[:, 0:1826].loc[:, double_KO.columns[1:]]\n",
    "        x_valid = X_valid_liq.iloc[:, 0:1826].loc[:, double_KO.columns[1:]]\n",
    "\n",
    "        loaded_model = joblib.load(\"svm_reg_t_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model.best_estimator_, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"lasso_reg_t_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "\n",
    "        transcriptome = X_train_liq.iloc[:, 0:1826].loc[:, double_KO.columns[1:]]\n",
    "        flux = X_train_liq.iloc[:, 1826: 3776]\n",
    "        x_train = pd.concat([transcriptome, flux], axis=1)\n",
    "        x_valid = X_valid_liq.iloc[:, 0:3776].loc[:, x_train.columns]\n",
    "\n",
    "        loaded_model = joblib.load(\"svm_reg_t_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model.best_estimator_, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"lasso_reg_t_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        x_train = X_train_solid.iloc[:, 3776:5602].loc[:, double_KO.columns[1:]]\n",
    "        x_valid = X_valid_solid.iloc[:, 3776:5602].loc[:, double_KO.columns[1:]]\n",
    "        x_train.dropna(inplace=True)\n",
    "        x_valid.dropna(inplace=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"svm_reg_p_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model.best_estimator_, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"lasso_reg_p_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "\n",
    "        proteome = X_train_solid.iloc[:, 3776:5602].loc[:, double_KO.columns[1:]]\n",
    "        flux = X_train_solid.iloc[:, 5602: ]\n",
    "        x_train = pd.concat([proteome, flux], axis=1)\n",
    "        x_valid = X_valid_solid.iloc[:, 3776:].loc[:, x_train.columns]\n",
    "        x_train.dropna(inplace=True)\n",
    "        x_valid.dropna(inplace=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"svm_reg_p_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model.best_estimator_, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "        loaded_model = joblib.load(\"lasso_reg_p_f_glpk_\"+str(percent)+\".joblib\")\n",
    "        features = feature_importance(loaded_model, labels, x_valid)\n",
    "        coef = pd.concat([coef, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "\n",
    "        index = []\n",
    "        for data in ['t', 't_f', 'p', 'p_f']:\n",
    "                for j in ['svm', 'lasso_reg']:\n",
    "                        index.append(j+'_'+data+'_glpk_'+str(p))\n",
    "\n",
    "        coef.index = index\n",
    "        coef.to_csv(\"regressors_features_shap\"+str(s)+\".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
