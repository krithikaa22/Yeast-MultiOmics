{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from rgf.sklearn import RGFClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, median_absolute_error, root_mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import joblib\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "\n",
    "from tensorflow.keras import backend as F\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Concatenate, Input\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to load the datasets, preprocess and format the data\n",
    "\n",
    "\n",
    "Input: None\n",
    "\n",
    "\n",
    "Output: Return the preprocessed dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "\n",
    "    #load T and P\n",
    "    t = pd.read_csv('./transcriptome/compare_transcriptome.csv')\n",
    "    p = pd.read_csv('./proteome/compare_proteome.csv')\n",
    "\n",
    "    #remove the NaN values from P\n",
    "    p.dropna(inplace=True)\n",
    "\n",
    "    #set the knockout name as the index\n",
    "    t.index = t['Unnamed: 0']\n",
    "    p.index = p['Unnamed: 0']\n",
    "\n",
    "    #remove the knockout name column from the dataframe\n",
    "    t = t.iloc[:, 1:]\n",
    "    p = p.iloc[:, 2:]\n",
    "\n",
    "    #load the fluxomics datasets and set the index same as T or P\n",
    "    fluxes_p = pd.read_csv('./fluxes/Fluxes_matlab_seed1.csv', header=None)\n",
    "    fluxes_p = fluxes_p.T\n",
    "    fluxes_p.index = p.index \n",
    "\n",
    "    fluxes_t = pd.read_csv('./fluxes/Fluxes_t_matlab.csv', header=None)\n",
    "    fluxes_t  = fluxes_t.T\n",
    "    fluxes_t.index = t.index\n",
    "\n",
    "    #min-max normalization of fluxomics dataframe\n",
    "    X_train_liq_pf = (fluxes_p - fluxes_p.min()) / fluxes_p.max() - fluxes_p.min()\n",
    "    fluxes_p = X_train_liq_pf\n",
    "\n",
    "    X_valid_liq_pf = (fluxes_t - fluxes_t.min()) / fluxes_t.max() - fluxes_t.min()\n",
    "    fluxes_t = X_valid_liq_pf\n",
    "\n",
    "    #remove the NaN columns from the normalized fluxomics dataframe\n",
    "    fluxes_p.dropna(axis=1, inplace=True)\n",
    "    fluxes_t.dropna(axis=1, inplace=True)\n",
    "\n",
    "    #min-max normalization of T and P dataframe\n",
    "    p = (p - p.min())/(p.max() - p.min())\n",
    "    t = (t - t.min())/(t.max() - t.min())\n",
    "\n",
    "    #load the growth rates and set the index same as T or P\n",
    "    gr_measured_ML = pd.read_csv('./fluxes/compare_transcriptome_measured_gr.csv')\n",
    "    gr_measured_ML.index = t.index\n",
    "\n",
    "    y = pd.read_csv('./fluxes/compare_measured_gr.csv')\n",
    "    y.index = y['ORF PROT']\n",
    "\n",
    "    return [t, fluxes_t, p, fluxes_p, gr_measured_ML, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to filter knockouts as low or high growth based on a given percentile\n",
    "\n",
    "\n",
    "Input: Liquid GR, Solid GR, Percentile for splitting\n",
    "\n",
    "\n",
    "Output: Modified GR dataframe with a class column [0 as low and 2 as high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ko(gr_measured_ML, y, a_per, b_per):\n",
    "    \n",
    "    #find the percentile for liquid GR\n",
    "    a = np.percentile(gr_measured_ML['0'], a_per)\n",
    "    b = np.percentile(gr_measured_ML['0'], b_per)\n",
    "\n",
    "    y_t = []\n",
    "    for i in gr_measured_ML['0']:\n",
    "        if i < a:\n",
    "            y_t.append(0)\n",
    "        elif i>a and i<b:\n",
    "            y_t.append(1)\n",
    "        else:\n",
    "            y_t.append(2)\n",
    "\n",
    "    #append class column based on the calculated percentile\n",
    "    gr_measured_ML['class'] = y_t\n",
    "\n",
    "\n",
    "    #find the percentile for solid GR\n",
    "    a = np.percentile(y['SM'], a_per)\n",
    "    b = np.percentile(y['SM'], b_per)\n",
    "\n",
    "    y_p = []\n",
    "    for i in y['SM']:\n",
    "        if i < a:\n",
    "            y_p.append(0)\n",
    "        elif i>a and i<b:\n",
    "            y_p.append(1)\n",
    "        else:\n",
    "            y_p.append(2)\n",
    "\n",
    "    #append class column based on the calculated percentile\n",
    "    y['class'] = y_p\n",
    "\n",
    "    #filter medium gr KOs\n",
    "    gr_measured_ML = gr_measured_ML[gr_measured_ML['class'] != 1]\n",
    "    y = y[y['class'] != 1]\n",
    "\n",
    "    return [gr_measured_ML, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to combine the transcriptomics, proteomics and fluxomics dataset into a single dataframe\n",
    "\n",
    "\n",
    "Input: Transcriptomics, Proteomics, Fluxomics derived from transcriptomics, Fluxomics derived from proteomics\n",
    "\n",
    "\n",
    "Output: Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combined_dataset(t, p, fluxes_t, fluxes_p):\n",
    "\n",
    "    X = pd.concat([t, fluxes_t, p, fluxes_p], axis=1)\n",
    "    X.columns = X.columns.astype(str)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to split the dataset into train, test and validation \n",
    "\n",
    "Input: y dataframes\n",
    "\n",
    "Output: Train, test and validation dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(y):\n",
    "\n",
    "    # est: 30% of the total data\n",
    "    test_size = int(0.3*y.shape[0])\n",
    "\n",
    "    #train: 80% of the remaining data\n",
    "    train_size = int(0.8*(y.shape[0] - test_size))\n",
    "\n",
    "    valid_size = int(0.2*(y.shape[0] - test_size))\n",
    "\n",
    "    #randomly sample train_size number of rows, ensuring equal split of each class\n",
    "    y_train = y.groupby('class', group_keys=False).apply(lambda x: x.sample(n=train_size//2, random_state=42))\n",
    "\n",
    "    #filter the remaining indices\n",
    "    train_valid = y.loc[list(set(y.index).difference(set(y_train.index))), :]\n",
    "\n",
    "    #randomly sample valid_size number of rows from the remaining rows\n",
    "    y_valid = train_valid.groupby('class', group_keys=False).apply(lambda x: x.sample(n=valid_size//2, random_state=42))\n",
    "\n",
    "    y_test = y.loc[list(set(y.index).difference(set(y_valid.index).union(set(y_train.index)))), :]\n",
    "\n",
    "    return [y_train, y_test, y_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to find performance of given model\n",
    "\n",
    "\n",
    "Input: Model, X train values, y train values, X test values, y test values, title for the plots\n",
    "\n",
    "\n",
    "Output: Array of statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(model, X_train, y_train, X_test, y_test, condition, ann = False):\n",
    "\n",
    "    #predict y values with the test split\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    if ann == True:\n",
    "        pred = [2 if i > np.mean(y_pred) else 0 for i in y_pred ]\n",
    "        y_pred = pred\n",
    "        print(y_pred)\n",
    "\n",
    "    #print statistics\n",
    "    print('R2 score: ', r2_score(y_test, y_pred))\n",
    "    print('MAE: ', mean_absolute_error(y_test, y_pred))\n",
    "    print('RMSE: ', root_mean_squared_error(y_test, y_pred))\n",
    "    print('MDAE: ', median_absolute_error(y_test, y_pred))\n",
    "    print(\"Spearman Correlation: \", stats.spearmanr(y_test, y_pred))\n",
    "\n",
    "    statistics = [r2_score(y_test, y_pred), mean_absolute_error(y_test, y_pred), root_mean_squared_error(y_test, y_pred), median_absolute_error(y_test, y_pred), stats.spearmanr(y_test, y_pred)]\n",
    "\n",
    "    #print accuracy\n",
    "    if ann == True:\n",
    "        \n",
    "        print('Accuracy: ',accuracy_score(y_pred, y_test))\n",
    "        pred = model.predict(X_train)\n",
    "        train_acc = accuracy_score([2 if i > np.mean(pred) else 0 for i in pred ], y_train)\n",
    "        print(\"Train accuracy: \", train_acc)\n",
    "    else:\n",
    "        print('Accuracy: ',accuracy_score(y_pred, y_test))\n",
    "        train_acc = accuracy_score(model.predict(X_train), y_train)\n",
    "        print(\"Train accuracy: \", train_acc)\n",
    "\n",
    "    statistics.append(accuracy_score(y_pred, y_test))\n",
    "    statistics.append(train_acc)\n",
    "\n",
    "    #print classification report [precision    recall  f1-score   support]\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    statistics.append(classification_report(y_test, y_pred, output_dict=True)['weighted avg'])\n",
    "\n",
    "    #find the classification from prediction of both train and validation\n",
    "\n",
    "    df_aux = pd.concat([X_test, y_test], axis=1)\n",
    "    df_aux['class'] = [1 if y == 0 else 0 for y in y_test.values]\n",
    "    pred = model.predict(X_test)\n",
    "    if ann:\n",
    "        pred = [2 if i > np.mean(pred) else 0 for i in pred]\n",
    "    df_aux['prob'] = [1 if y == 0 else 0 for y in pred]\n",
    "    df_aux = df_aux.reset_index(drop = True)\n",
    "\n",
    "    df_aux_train = pd.concat([X_train, y_train], axis=1)\n",
    "    df_aux_train['class'] = [1 if y == 0 else 0 for y in y_train.values]\n",
    "    pred = model.predict(X_train)\n",
    "    if ann:\n",
    "        pred = [2 if i > np.mean(pred) else 0 for i in pred]\n",
    "    df_aux_train['prob'] = [1 if y == 0 else 0 for y in pred]\n",
    "    df_aux_train = df_aux_train.reset_index(drop = True)\n",
    "\n",
    "    #find the false positive rate\n",
    "    fpr, tpr, thresholds = roc_curve(df_aux['class'], df_aux['prob']) \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    fpr_train, tpr_train, thresholds_train = roc_curve(df_aux_train['class'], df_aux_train['prob']) \n",
    "    roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "\n",
    "    #plot the ROC curve for both validation and training\n",
    "    plt.figure()  \n",
    "    plt.plot(fpr, tpr, label='Validation ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot(fpr_train, tpr_train, label='Train ROC curve (area = %0.2f)' % roc_auc_train)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(condition)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train and test the performance of asn SVM classifier\n",
    "\n",
    "\n",
    "Input: X train values, X validation values, y train values, y validation values, name to save the model, dataframe to store the statistics\n",
    "\n",
    "Output: Returns the statistics dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_model(x_train, x_valid, y_train, y_valid, name, temp):\n",
    "\n",
    "    #initialize a SVM classifier\n",
    "    svm = SVC(random_state = 11850)\n",
    "\n",
    "    #set the hyperparameters\n",
    "    param_dist = {'C': [0.1, 0.5, 1, 5, 10, 15, 20, 25, 30, 35],\n",
    "                    'gamma': [0.001, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1],\n",
    "                    'kernel': ['linear', 'rbf', 'poly']\n",
    "                }\n",
    "    \n",
    "    #perform gridsearch to find the optimal hyperparameters\n",
    "    svm = GridSearchCV(svm, param_dist, cv=10, return_train_score=True)\n",
    "\n",
    "    #fit the best classifier and save the model\n",
    "    svm.fit(x_train, y_train)\n",
    "    joblib.dump(svm, name+\".joblib\")\n",
    "\n",
    "    #test the performance of the model\n",
    "    statistics = performance(svm, x_train, y_train, x_valid, y_valid, name)\n",
    "\n",
    "    #find the index of the best estimator\n",
    "    best_estimator = {\n",
    "            'C': svm.best_estimator_.C,\n",
    "        }\n",
    "\n",
    "    index = 0\n",
    "    for i in range(len(svm.cv_results_['params'])):\n",
    "            if svm.cv_results_['params'][i] == best_estimator:\n",
    "                index = i\n",
    "                break\n",
    "\n",
    "    #filter the training score across 10 folds of the best estimator\n",
    "    scores = [svm.cv_results_['split'+str(i)+'_train_score'][index] for i in range(10)]\n",
    "    print(scores)\n",
    "\n",
    "    #plot the score across the 10 folds\n",
    "    plt.plot(scores)\n",
    "    plt.title(name + \"k-fold\")\n",
    "\n",
    "    temp.loc[name, temp.columns[:4]] = statistics[:4]\n",
    "    temp.loc[name, 'Spearman Correlation'] = statistics[4].statistic\n",
    "    temp.loc[name, temp.columns[5:7]] = statistics[5:7]\n",
    "    temp.loc[name, 'Precision'] = statistics[7]['precision']\n",
    "    temp.loc[name, 'Recall'] = statistics[7]['recall']\n",
    "    temp.loc[name, 'F1-Score'] = statistics[7]['f1-score']\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_Lasso_model(x_train, x_valid, y_train, y_valid, name, temp):\n",
    "\n",
    "    #initialize a Logistic Regressor classifier with LASSO regularization\n",
    "    logreg = LogisticRegression(penalty='l2')\n",
    "\n",
    "    #set the hyperparameters\n",
    "    grid={\"C\": [0.0, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0, 2.25, 2.5]}\n",
    "    \n",
    "    #perform gridsearch to find the optimal hyperparameters\n",
    "    logreg_cv = GridSearchCV(logreg,grid,cv=10)\n",
    "\n",
    "    #fit the best classifier and save the model\n",
    "    logreg_cv.fit(x_train, y_train)\n",
    "    joblib.dump(logreg_cv, name+\".joblib\")\n",
    "\n",
    "    #test the performance of the model\n",
    "    statistics = performance(logreg_cv, x_train, y_train, x_valid, y_valid, name)\n",
    "\n",
    "    #find the index of the best estimator\n",
    "    best_estimator = {\n",
    "            'C': logreg_cv.best_estimator_.C,\n",
    "        }\n",
    "\n",
    "    index = 0\n",
    "    for i in range(len(logreg_cv.cv_results_['params'])):\n",
    "            if logreg_cv.cv_results_['params'][i] == best_estimator:\n",
    "                index = i\n",
    "                break\n",
    "\n",
    "    #filter the training score across 10 folds of the best estimator\n",
    "    scores = [logreg_cv.cv_results_['split'+str(i)+'_test_score'][index] for i in range(10)]\n",
    "    print(scores)\n",
    "\n",
    "    #plot the score across the 10 folds\n",
    "    plt.plot(scores)\n",
    "    plt.title(name + \"k-fold\")\n",
    "\n",
    "    temp.loc[name, temp.columns[:4]] = statistics[:4]\n",
    "    temp.loc[name, 'Spearman Correlation'] = statistics[4].statistic\n",
    "    temp.loc[name, temp.columns[5:7]] = statistics[5:7]\n",
    "    temp.loc[name, 'Precision'] = statistics[7]['precision']\n",
    "    temp.loc[name, 'Recall'] = statistics[7]['recall']\n",
    "    temp.loc[name, 'F1-Score'] = statistics[7]['f1-score']\n",
    "\n",
    "    return temp\n",
    "\n",
    "def LR_elasticnet_model(x_train, x_valid, y_train, y_valid, name, temp):\n",
    "\n",
    "    #initialize a Logistic Regressor classifier with LASSO regularization\n",
    "    logreg = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)\n",
    "\n",
    "    #set the hyperparameters\n",
    "    grid={\"C\":np.logspace(-5,5,50)}\n",
    "    \n",
    "    #perform gridsearch to find the optimal hyperparameters\n",
    "    logreg_cv = GridSearchCV(logreg,grid,cv=10)\n",
    "\n",
    "    #fit the best classifier and save the model\n",
    "    logreg_cv.fit(x_train, y_train)\n",
    "    joblib.dump(logreg_cv, name+\".joblib\")\n",
    "\n",
    "    #test the performance of the model\n",
    "    statistics = performance(logreg_cv, x_train, y_train, x_valid, y_valid, name)\n",
    "\n",
    "    #find the index of the best estimator\n",
    "    best_estimator = {\n",
    "            'C': logreg_cv.best_estimator_.C,\n",
    "        }\n",
    "\n",
    "    index = 0\n",
    "    for i in range(len(logreg_cv.cv_results_['params'])):\n",
    "            if logreg_cv.cv_results_['params'][i] == best_estimator:\n",
    "                index = i\n",
    "                break\n",
    "\n",
    "    #filter the training score across 10 folds of the best estimator\n",
    "    scores = [logreg_cv.cv_results_['split'+str(i)+'_test_score'][index] for i in range(10)]\n",
    "    print(scores)\n",
    "\n",
    "    #plot the score across the 10 folds\n",
    "    plt.plot(scores)\n",
    "    plt.title(name + \"k-fold\")\n",
    "\n",
    "    temp.loc[name, temp.columns[:4]] = statistics[:4]\n",
    "    temp.loc[name, 'Spearman Correlation'] = statistics[4].statistic\n",
    "    temp.loc[name, temp.columns[5:7]] = statistics[5:7]\n",
    "    temp.loc[name, 'Precision'] = statistics[7]['precision']\n",
    "    temp.loc[name, 'Recall'] = statistics[7]['recall']\n",
    "    temp.loc[name, 'F1-Score'] = statistics[7]['f1-score']\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest_model(x_train, x_valid, y_train, y_valid, name, temp):\n",
    "\n",
    "    #initialize a Random Forest classifier\n",
    "    rf = RandomForestClassifier(random_state = 11850)\n",
    "\n",
    "    #set the hyperparameters\n",
    "    param_dist = {'n_estimators': list(range(200, 500, 50)),\n",
    "              'max_depth': list(range(1,15,2)),\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'max_features': list(range(120, 250, 10))\n",
    "            }\n",
    "    \n",
    "    #perform gridsearch to find the optimal hyperparameters\n",
    "    rf = GridSearchCV(rf, param_dist, cv=10, return_train_score=True)\n",
    "\n",
    "    #fit the best classifier and save the model\n",
    "    rf.fit(x_train, y_train)\n",
    "    joblib.dump(rf, name+\".joblib\")\n",
    "\n",
    "    #test the performance of the model\n",
    "    statistics = performance(rf, x_train, y_train, x_valid, y_valid, name)\n",
    "\n",
    "    #find the index of the best estimator\n",
    "    best_estimator = {\n",
    "            'n_estimators': rf.best_estimator_.n_estimators,\n",
    "            'max_depth': rf.best_estimator_.max_depth,\n",
    "            'min_samples_leaf': rf.best_estimator_.min_samples_leaf,\n",
    "            'max_features': rf.best_estimator_.max_features,\n",
    "        }\n",
    "\n",
    "    index = 0\n",
    "    for i in range(len(rf.cv_results_['params'])):\n",
    "            if rf.cv_results_['params'][i] == best_estimator:\n",
    "                index = i\n",
    "                break\n",
    "\n",
    "    #filter the training score across 10 folds of the best estimator\n",
    "    scores = [rf.cv_results_['split'+str(i)+'_train_score'][index] for i in range(10)]\n",
    "    print(scores)\n",
    "\n",
    "    #plot the score across the 10 folds\n",
    "    plt.plot(scores)\n",
    "    plt.title(name + \"k-fold\")\n",
    "\n",
    "    temp.loc[name, temp.columns[:4]] = statistics[:4]\n",
    "    temp.loc[name, 'Spearman Correlation'] = statistics[4].statistic\n",
    "    temp.loc[name, temp.columns[5:7]] = statistics[5:7]\n",
    "    temp.loc[name, 'Precision'] = statistics[7]['precision']\n",
    "    temp.loc[name, 'Recall'] = statistics[7]['recall']\n",
    "    temp.loc[name, 'F1-Score'] = statistics[7]['f1-score']\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN_model(x_train, x_valid, y_train, y_valid, name, temp):\n",
    "        \n",
    "        #initialize a kNN classifier\n",
    "        knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "        #set the hyperparameters\n",
    "        parameters_KNN = {\n",
    "            'n_neighbors': (1,50, 5),\n",
    "            'leaf_size': (20,100,10),\n",
    "            'p': (1,2),\n",
    "            'weights': ('uniform', 'distance'),\n",
    "            'metric': ('minkowski', 'chebyshev')}\n",
    "        \n",
    "        #perform gridsearch to find the optimal hyperparameters\n",
    "        knn = GridSearchCV(\n",
    "                estimator=knn,\n",
    "                param_grid=parameters_KNN,\n",
    "                scoring = 'accuracy',\n",
    "                n_jobs = -1,\n",
    "                cv = 5\n",
    "            )\n",
    "\n",
    "        #fit the best classifier and save the model\n",
    "        knn.fit(x_train, y_train)\n",
    "        joblib.dump(knn, name+\".joblib\")\n",
    "\n",
    "        #test the performance of the model\n",
    "        statistics = performance(knn, x_train, y_train, x_valid, y_valid, name)\n",
    "\n",
    "        #find the index of the best estimator\n",
    "        best_estimator = {\n",
    "                'n_neighbors': knn.best_estimator_.n_neighbors,\n",
    "                'leaf_size': knn.best_estimator_.leaf_size,\n",
    "                'p': knn.best_estimator_.p,\n",
    "                'weights': knn.best_estimator_.weights,\n",
    "                'metric': knn.best_estimator_.metric,\n",
    "            }\n",
    "\n",
    "        index = 0\n",
    "        for i in range(len(knn.cv_results_['params'])):\n",
    "                if knn.cv_results_['params'][i] == best_estimator:\n",
    "                    index = i\n",
    "                    break\n",
    "\n",
    "        #filter the training score across 10 folds of the best estimator\n",
    "        scores = [knn.cv_results_['split'+str(i)+'_train_score'][index] for i in range(10)]\n",
    "        print(scores)\n",
    "\n",
    "        #plot the score across the 10 folds\n",
    "        plt.plot(scores)\n",
    "        plt.title(name + \"k-fold\")\n",
    "\n",
    "        temp.loc[name, temp.columns[:4]] = statistics[:4]\n",
    "        temp.loc[name, 'Spearman Correlation'] = statistics[4].statistic\n",
    "        temp.loc[name, temp.columns[5:7]] = statistics[5:7]\n",
    "        temp.loc[name, 'Precision'] = statistics[7]['precision']\n",
    "        temp.loc[name, 'Recall'] = statistics[7]['recall']\n",
    "        temp.loc[name, 'F1-Score'] = statistics[7]['f1-score']\n",
    "\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_LASSO_model(x_train, x_valid, y_train, y_valid, name, temp):\n",
    "        \n",
    "        #initialize a Linear Regressor with LASSO regularization\n",
    "        reg = Lasso()\n",
    "\n",
    "        #set the hyperparameters\n",
    "        param_space = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0]}\n",
    "        \n",
    "        #perform gridsearch to find the optimal hyperparameters\n",
    "        reg = GridSearchCV(reg, param_space, cv=10)\n",
    "\n",
    "        #fit the best classifier and save the model\n",
    "        reg.fit(x_train, y_train)\n",
    "        joblib.dump(reg, name+\".joblib\")\n",
    "\n",
    "        #test the performance of the model\n",
    "        statistics = performance(reg, x_train, y_train, x_valid, y_valid, name)\n",
    "\n",
    "        #find the index of the best estimator\n",
    "        best_estimator = {\n",
    "                'alpha': reg.best_estimator_.alpha,\n",
    "            }\n",
    "\n",
    "        index = 0\n",
    "        for i in range(len(reg.cv_results_['params'])):\n",
    "                if reg.cv_results_['params'][i] == best_estimator:\n",
    "                    index = i\n",
    "                    break\n",
    "\n",
    "        #filter the training score across 10 folds of the best estimator\n",
    "        scores = [reg.cv_results_['split'+str(i)+'_test_score'][index] for i in range(10)]\n",
    "        print(scores)\n",
    "\n",
    "        #plot the score across the 10 folds\n",
    "        plt.plot(scores)\n",
    "        plt.title(name + \"k-fold\")\n",
    "\n",
    "        temp.loc[name, temp.columns[:4]] = statistics[:4]\n",
    "        temp.loc[name, 'Spearman Correlation'] = statistics[4].statistic\n",
    "        temp.loc[name, temp.columns[5:7]] = statistics[5:7]\n",
    "        temp.loc[name, 'Precision'] = statistics[7]['precision']\n",
    "        temp.loc[name, 'Recall'] = statistics[7]['recall']\n",
    "        temp.loc[name, 'F1-Score'] = statistics[7]['f1-score']\n",
    "\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_RandomForest_model(x_train, x_valid, y_train, y_valid, name, temp):\n",
    "        \n",
    "        #initialize a Linear Regressor with LASSO regularization\n",
    "        reg = RandomForestRegressor()\n",
    "\n",
    "        #set the hyperparameters\n",
    "        param_dist = {'n_estimators': list(range(200, 500, 50)),\n",
    "              'max_depth': list(range(1,15,2)),\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'max_features': list(range(120, 250, 10))\n",
    "            }\n",
    "        \n",
    "        #perform gridsearch to find the optimal hyperparameters\n",
    "        reg = GridSearchCV(reg, param_dist, cv=10)\n",
    "\n",
    "        #fit the best classifier and save the model\n",
    "        reg.fit(x_train, y_train)\n",
    "        joblib.dump(reg, name+\".joblib\")\n",
    "\n",
    "        #test the performance of the model\n",
    "        statistics = performance(reg, x_train, y_train, x_valid, y_valid, name)\n",
    "\n",
    "        #find the index of the best estimator\n",
    "        best_estimator = {\n",
    "            'n_estimators': reg.best_estimator_.n_estimators,\n",
    "            'max_depth': reg.best_estimator_.max_depth,\n",
    "            'min_samples_leaf': reg.best_estimator_.min_samples_leaf,\n",
    "            'max_features': reg.best_estimator_.max_features,\n",
    "        }\n",
    "\n",
    "        index = 0\n",
    "        for i in range(len(reg.cv_results_['params'])):\n",
    "                if reg.cv_results_['params'][i] == best_estimator:\n",
    "                    index = i\n",
    "                    break\n",
    "\n",
    "        #filter the training score across 10 folds of the best estimator\n",
    "        scores = [reg.cv_results_['split'+str(i)+'_train_score'][index] for i in range(10)]\n",
    "        print(scores)\n",
    "\n",
    "        #plot the score across the 10 folds\n",
    "        plt.plot(scores)\n",
    "        plt.title(name + \"k-fold\")\n",
    "\n",
    "        temp.loc[name, temp.columns[:4]] = statistics[:4]\n",
    "        temp.loc[name, 'Spearman Correlation'] = statistics[4].statistic\n",
    "        temp.loc[name, temp.columns[5:7]] = statistics[5:7]\n",
    "        temp.loc[name, 'Precision'] = statistics[7]['precision']\n",
    "        temp.loc[name, 'Recall'] = statistics[7]['recall']\n",
    "        temp.loc[name, 'F1-Score'] = statistics[7]['f1-score']\n",
    "\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_SVM_model(x_train, x_valid, y_train, y_valid, name, temp):\n",
    "        \n",
    "        #initialize a SVM Regressor\n",
    "        reg = SVR()\n",
    "\n",
    "        #set the hyperparameters\n",
    "        param_dist = {'C': [0.1, 0.5, 1, 5, 10, 15, 20, 25, 30, 35],\n",
    "                    'gamma': [0.001, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1],\n",
    "                    'kernel': ['linear', 'rbf', 'poly']\n",
    "                }\n",
    "        \n",
    "        #perform gridsearch to find the optimal hyperparameters\n",
    "        reg = GridSearchCV(reg, param_dist, cv=10)\n",
    "\n",
    "        #fit the best classifier and save the model\n",
    "        reg.fit(x_train, y_train)\n",
    "        joblib.dump(reg, name+\".joblib\")\n",
    "\n",
    "        #test the performance of the model\n",
    "        statistics = performance(reg, x_train, y_train, x_valid, y_valid, name)\n",
    "\n",
    "        #find the index of the best estimator\n",
    "        best_estimator = {\n",
    "            'C': svm.best_estimator_.C,\n",
    "        }\n",
    "\n",
    "        index = 0\n",
    "        for i in range(len(reg.cv_results_['params'])):\n",
    "                if reg.cv_results_['params'][i] == best_estimator:\n",
    "                    index = i\n",
    "                    break\n",
    "\n",
    "        #filter the training score across 10 folds of the best estimator\n",
    "        scores = [reg.cv_results_['split'+str(i)+'_train_score'][index] for i in range(10)]\n",
    "        print(scores)\n",
    "\n",
    "        #plot the score across the 10 folds\n",
    "        plt.plot(scores)\n",
    "        plt.title(name + \"k-fold\")\n",
    "\n",
    "        temp.loc[name, temp.columns[:4]] = statistics[:4]\n",
    "        temp.loc[name, 'Spearman Correlation'] = statistics[4].statistic\n",
    "        temp.loc[name, temp.columns[5:7]] = statistics[5:7]\n",
    "        temp.loc[name, 'Precision'] = statistics[7]['precision']\n",
    "        temp.loc[name, 'Recall'] = statistics[7]['recall']\n",
    "        temp.loc[name, 'F1-Score'] = statistics[7]['f1-score']\n",
    "\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the external double KO dataset [required for the features list]\n",
    "\n",
    "double_KO = pd.read_csv(\"../Krithikaa/yeast_datasets/yeast_gstf_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_46332/3380375233.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_train = y.groupby('class', group_keys=False).apply(lambda x: x.sample(n=train_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_46332/3380375233.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_valid = train_valid.groupby('class', group_keys=False).apply(lambda x: x.sample(n=valid_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_46332/3380375233.py:12: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_train = y.groupby('class', group_keys=False).apply(lambda x: x.sample(n=train_size//2, random_state=42))\n",
      "/var/folders/1g/phgxzfj511q1zyh71ggb7hxw0000gq/T/ipykernel_46332/3380375233.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y_valid = train_valid.groupby('class', group_keys=False).apply(lambda x: x.sample(n=valid_size//2, random_state=42))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 38\u001b[0m\n\u001b[1;32m     34\u001b[0m x_train \u001b[38;5;241m=\u001b[39m X_train_liq\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1826\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[:, double_KO\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m1\u001b[39m:]]\n\u001b[1;32m     35\u001b[0m x_valid \u001b[38;5;241m=\u001b[39m X_valid_liq\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1826\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[:, double_KO\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m1\u001b[39m:]]\n\u001b[0;32m---> 38\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43mSVM_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_liq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid_liq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msvm_t_30\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m temp \u001b[38;5;241m=\u001b[39m LR_Lasso_model(x_train, x_valid, y_train_liq, y_valid_liq, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlasso_log_t_30\u001b[39m\u001b[38;5;124m'\u001b[39m, temp)\n\u001b[1;32m     40\u001b[0m temp \u001b[38;5;241m=\u001b[39m LR_elasticnet_model(x_train, x_valid, y_train_liq, y_valid_liq, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124melasticnet_log_t_30\u001b[39m\u001b[38;5;124m'\u001b[39m, temp)\n",
      "Cell \u001b[0;32mIn[86], line 16\u001b[0m, in \u001b[0;36mSVM_model\u001b[0;34m(x_train, x_valid, y_train, y_valid, name, temp)\u001b[0m\n\u001b[1;32m     13\u001b[0m svm \u001b[38;5;241m=\u001b[39m GridSearchCV(svm, param_dist, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#fit the best classifier and save the model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43msvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(svm, name\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#test the performance of the model\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Krithikaa/fluxes/.venv/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Krithikaa/fluxes/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Krithikaa/fluxes/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Krithikaa/fluxes/.venv/lib/python3.11/site-packages/sklearn/model_selection/_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m     )\n\u001b[0;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Krithikaa/fluxes/.venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Krithikaa/fluxes/.venv/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Desktop/Krithikaa/fluxes/.venv/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Desktop/Krithikaa/fluxes/.venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Krithikaa/fluxes/.venv/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Desktop/Krithikaa/fluxes/.venv/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Krithikaa/fluxes/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[0;32m--> 250\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[0;32m~/Desktop/Krithikaa/fluxes/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:328\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    314\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[1;32m    318\u001b[0m (\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[0;32m--> 328\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass_weight_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#load and preprocess data\n",
    "[t, fluxes_t, p, fluxes_p, gr_measured_ML, y] = load_dataset()\n",
    "\n",
    "#filter based on percentile\n",
    "[gr_measured_ML, y] = filter_ko(gr_measured_ML, y, 30, 70)\n",
    "\n",
    "#concatenate the dataframes\n",
    "X = generate_combined_dataset(t, p, fluxes_t, fluxes_p)\n",
    "\n",
    "#split transcriptomics dataset into train, test and validation\n",
    "[y_train_liq, y_test_liq, y_valid_liq] = split_data(gr_measured_ML)\n",
    "X_train_liq = X.loc[y_train_liq.index, :]\n",
    "X_test_liq = X.loc[y_test_liq.index, :]\n",
    "X_valid_liq = X.loc[y_valid_liq.index, :]\n",
    "\n",
    "y_valid_liq = y_valid_liq['class']\n",
    "y_train_liq = y_train_liq['class']\n",
    "y_test_liq = y_test_liq['class']\n",
    "\n",
    "#split proteomics dataset into train, test and validation\n",
    "[y_train_solid, y_test_solid, y_valid_solid] = split_data(y)\n",
    "X_train_solid = X.loc[y_train_solid.index, :]\n",
    "X_test_solid = X.loc[y_test_solid.index, :]\n",
    "X_valid_solid = X.loc[y_valid_solid.index, :]\n",
    "\n",
    "y_train_solid = y_train_solid['class']\n",
    "y_test_solid = y_test_solid['class']\n",
    "y_valid_solid = y_valid_solid['class']\n",
    "\n",
    "#initialize the statistics dataframe\n",
    "temp = pd.DataFrame(columns=['R2 Score', 'MAE', 'RMSE', 'MDAE', 'Spearman Correlation', 'Accuracy', 'Train Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "#subset train and validation for transcriptomics\n",
    "x_train = X_train_liq.iloc[:, 0:1826].loc[:, double_KO.columns[1:]]\n",
    "x_valid = X_valid_liq.iloc[:, 0:1826].loc[:, double_KO.columns[1:]]\n",
    "\n",
    "\n",
    "temp = SVM_model(x_train, x_valid, y_train_liq, y_valid_liq, 'svm_t_30', temp)\n",
    "temp = LR_Lasso_model(x_train, x_valid, y_train_liq, y_valid_liq, 'lasso_log_t_30', temp)\n",
    "temp = LR_elasticnet_model(x_train, x_valid, y_train_liq, y_valid_liq, 'elasticnet_log_t_30', temp)\n",
    "temp = kNN_model(x_train, x_valid, y_train_liq, y_valid_liq, 'knn_t_30', temp)\n",
    "temp = regression_LASSO_model(x_train, x_valid, y_train_liq, y_valid_liq, 'lasso_t_30', temp)\n",
    "temp = regression_SVM_model(x_train, x_valid, y_train_liq, y_valid_liq, 'svr_t_30', temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((991, 1826), (991, 396))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcriptome = X_train_liq.iloc[:, 0:1826].loc[:, double_KO.columns[1:]]\n",
    "flux = X_train_liq.iloc[:, 1826: 2222]\n",
    "x_train = pd.concat([transcriptome, flux], axis=1)\n",
    "x_valid = X_valid_liq.iloc[:, 0:2222].loc[:, x_train.columns]\n",
    "\n",
    "temp = SVM_model(x_train, x_valid, y_train_liq, y_valid_liq, 'svm_t_f_30', temp)\n",
    "temp = LR_Lasso_model(x_train, x_valid, y_train_liq, y_valid_liq, 'lasso_log_t_f_30', temp)\n",
    "temp = LR_elasticnet_model(x_train, x_valid, y_train_liq, y_valid_liq, 'elasticnet_log_t_f_30', temp)\n",
    "temp = kNN_model(x_train, x_valid, y_train_liq, y_valid_liq, 'knn_t_f_30', temp)\n",
    "temp = regression_LASSO_model(x_train, x_valid, y_train_liq, y_valid_liq, 'lasso_t_f_30', temp)\n",
    "temp = regression_SVM_model(x_train, x_valid, y_train_liq, y_valid_liq, 'svr_t_f_30', temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train_liq.iloc[:, 2222:4048].loc[:, double_KO.columns[1:]]\n",
    "x_valid = X_valid_liq.iloc[:, 2222:4048].loc[:, double_KO.columns[1:]]\n",
    "\n",
    "temp = SVM_model(x_train, x_valid, y_train_liq, y_valid_liq, 'svm_p_30', temp)\n",
    "temp = LR_Lasso_model(x_train, x_valid, y_train_liq, y_valid_liq, 'lasso_log_p_30', temp)\n",
    "temp = LR_elasticnet_model(x_train, x_valid, y_train_liq, y_valid_liq, 'elasticnet_log_p_30', temp)\n",
    "temp = kNN_model(x_train, x_valid, y_train_liq, y_valid_liq, 'knn_p_30', temp)\n",
    "temp = regression_LASSO_model(x_train, x_valid, y_train_liq, y_valid_liq, 'lasso_p_30', temp)\n",
    "temp = regression_SVM_model(x_train, x_valid, y_train_liq, y_valid_liq, 'svr_p_30', temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteome = X_train_liq.iloc[:, 2222:4048].loc[:, double_KO.columns[1:]]\n",
    "flux = X_train_liq.iloc[:, 4048: ]\n",
    "x_train = pd.concat([transcriptome, flux], axis=1)\n",
    "x_valid = X_valid_liq.iloc[:, 2222:].loc[:, x_train.columns]\n",
    "\n",
    "temp = SVM_model(x_train, x_valid, y_train_liq, y_valid_liq, 'svm_p_f_30', temp)\n",
    "temp = LR_Lasso_model(x_train, x_valid, y_train_liq, y_valid_liq, 'lasso_log_p_f_30', temp)\n",
    "temp = LR_elasticnet_model(x_train, x_valid, y_train_liq, y_valid_liq, 'elasticnet_log_p_f_30', temp)\n",
    "temp = kNN_model(x_train, x_valid, y_train_liq, y_valid_liq, 'knn_p_f_30', temp)\n",
    "temp = regression_LASSO_model(x_train, x_valid, y_train_liq, y_valid_liq, 'lasso_p_f_30', temp)\n",
    "temp = regression_SVM_model(x_train, x_valid, y_train_liq, y_valid_liq, 'svr_p_f_30', temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
